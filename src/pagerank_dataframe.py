"""
ImplÃ©mentation PageRank avec PySpark DataFrame
OptimisÃ© avec partitionnement intelligent et Catalyst optimizer
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as _sum, lit, explode, collect_list, size
import sys
import time
from utils import (
    parser_ligne_ttl,
    afficher_progression,
    mesurer_temps
)

def pagerank_dataframe(fichier_input, iterations=10, damping=0.85, num_partitions=200):
    """
    Calcul du PageRank avec PySpark DataFrame
    
    OPTIMISATIONS CLÃ‰S:
    1. Repartitionnement par clÃ© (source)
    2. Cache des DataFrames qui ne changent pas
    3. Utilisation de Catalyst optimizer
    4. Adaptive Query Execution
    
    Args:
        fichier_input: Chemin GCS vers les donnÃ©es TTL
        iterations: Nombre d'itÃ©rations PageRank
        damping: Facteur de damping (0.85 par dÃ©faut)
        num_partitions: Nombre de partitions
    
    Returns:
        Tuple (top_page, total_pages)
    """
    
    # CrÃ©er SparkSession avec optimisations
    spark = SparkSession.builder \
        .appName("PageRank-DataFrame") \
        .config("spark.sql.shuffle.partitions", str(num_partitions)) \
        .config("spark.default.parallelism", str(num_partitions)) \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.adaptive.skewJoin.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    print("\n" + "="*80)
    print("ğŸ”µ PageRank avec PySpark DataFrame")
    print("="*80)
    print(f"ğŸ“ Fichier d'entrÃ©e: {fichier_input}")
    print(f"ğŸ”„ ItÃ©rations: {iterations}")
    print(f"ğŸ“‰ Damping factor: {damping}")
    print(f"ğŸ“¦ Partitions: {num_partitions}")
    print(f"âš¡ Adaptive Query Execution: ActivÃ©")
    print("="*80 + "\n")
    
    # Ã‰tape 1: Charger et parser les donnÃ©es
    print("ğŸ“– Ã‰tape 1/5: Chargement et parsing des donnÃ©es...")
    debut_chargement = time.time()
    
    # Charger via RDD puis convertir en DataFrame
    lignes_rdd = spark.sparkContext.textFile(fichier_input)
    liens_rdd = lignes_rdd.map(parser_ligne_ttl).filter(lambda x: x is not None)
    
    fin_chargement = time.time()
    print(f"   âœ… Parsing effectuÃ© en {fin_chargement - debut_chargement:.2f} secondes\n")
    
    # Ã‰tape 2: Construire le graphe de liens (DataFrame)
    print("ğŸ”— Ã‰tape 2/5: Construction du graphe de liens (DataFrame)...")
    debut_graphe = time.time()
    
    # CrÃ©er DataFrame des liens
    df_liens_bruts = spark.createDataFrame(liens_rdd, ["source", "destination"])
    
    # Grouper les destinations par source
    # OPTIMISATION: Repartitionner et cacher
    df_liens = df_liens_bruts.groupBy("source") \
        .agg(collect_list("destination").alias("destinations")) \
        .repartition(num_partitions, "source") \
        .cache()  # â­ CACHE: Le graphe ne change jamais
    
    # Forcer l'Ã©valuation du cache
    num_pages = df_liens.count()
    
    fin_graphe = time.time()
    print(f"   âœ… Graphe construit en {fin_graphe - debut_graphe:.2f} secondes")
    
    # Statistiques
    print(f"\nğŸ“Š Statistiques du graphe:")
    print("-" * 60)
    print(f"   Pages avec liens sortants: {num_pages:,}")
    
    # Calculer le nombre moyen de liens
    avg_links = df_liens.agg(_sum(size("destinations")).alias("total")).collect()[0]["total"] / num_pages
    print(f"   Liens sortants - Moyenne: {avg_links:.2f}")
    print("-" * 60 + "\n")
    
    # Ã‰tape 3: Initialiser les rangs PageRank
    print("âš–ï¸  Ã‰tape 3/5: Initialisation des rangs PageRank...")
    
    # OPTIMISATION: Repartitionner de la mÃªme maniÃ¨re que les liens
    df_rangs = df_liens.select("source").distinct() \
        .withColumn("rank", lit(1.0)) \
        .repartition(num_partitions, "source")
    
    print(f"   âœ… {num_pages:,} pages initialisÃ©es avec rang = 1.0\n")
    
    # Ã‰tape 4: ItÃ©rations PageRank
    print("ğŸ”„ Ã‰tape 4/5: Calcul PageRank (itÃ©rations)...")
    print("-" * 80)
    
    debut_iterations = time.time()
    
    for iteration in range(iterations):
        # Afficher la progression
        afficher_progression(iteration + 1, iterations)
        
        # OPTIMISATION: Join sans shuffle (mÃªme clÃ© de partition)
        df_joint = df_liens.join(df_rangs, "source")
        
        # Calculer les contributions
        # Chaque page distribue son rang Ã©galement Ã  toutes ses destinations
        df_contributions = df_joint.select(
            explode("destinations").alias("destination"),
            (col("rank") / size("destinations")).alias("contribution")
        )
        
        # AgrÃ©ger les contributions et appliquer la formule PageRank
        # PageRank(p) = (1-d) + d * Î£(PR(in)/outlinks(in))
        df_rangs = df_contributions.groupBy("destination") \
            .agg(_sum("contribution").alias("rank_sum")) \
            .select(
                col("destination").alias("source"),
                (lit(damping) * col("rank_sum") + lit(1 - damping)).alias("rank")
            ) \
            .repartition(num_partitions, "source")  # Maintenir le partitionnement
    
    fin_iterations = time.time()
    temps_iterations = fin_iterations - debut_iterations
    
    print("-" * 80)
    print(f"âœ… {iterations} itÃ©rations terminÃ©es en {temps_iterations:.2f} secondes")
    print(f"   Temps moyen par itÃ©ration: {temps_iterations/iterations:.2f} secondes\n")
    
    # Ã‰tape 5: RÃ©sultats finaux
    print("ğŸ“Š Ã‰tape 5/5: Calcul des rÃ©sultats finaux...")
    
    # Cache pour les rÃ©sultats finaux
    df_rangs_final = df_rangs.cache()
    total_pages = df_rangs_final.count()
    
    print(f"   âœ… Total de pages analysÃ©es: {total_pages:,}\n")
    
    # Afficher le top 20
    print("=" * 80)
    print("ğŸ† Top 20 pages par PageRank")
    print("=" * 80 + "\n")
    
    top_pages_df = df_rangs_final.orderBy(col("rank").desc()).limit(20)
    top_pages_df.show(20, truncate=False)
    
    # RÃ©cupÃ©rer le top 1
    top_page = df_rangs_final.orderBy(col("rank").desc()).first()
    top_page_tuple = (top_page['source'], top_page['rank'])
    
    print("\n" + "=" * 80 + "\n")
    
    # Sauvegarder les rÃ©sultats dans GCS (format Parquet)
    output_path = fichier_input.replace('/data/', '/results/').replace('.ttl', '_dataframe_results')
    print(f"ğŸ’¾ Sauvegarde des rÃ©sultats dans: {output_path}")
    
    try:
        df_rangs_final.write.mode("overwrite").parquet(output_path)
        print("   âœ… RÃ©sultats sauvegardÃ©s (format Parquet)\n")
    except Exception as e:
        print(f"   âš ï¸  Erreur lors de la sauvegarde: {e}\n")
    
    # Sauvegarder aussi le top 100 en CSV pour lecture facile
    output_csv = output_path + "_top100.csv"
    try:
        df_rangs_final.orderBy(col("rank").desc()).limit(100) \
            .write.mode("overwrite").csv(output_csv, header=True)
        print(f"ğŸ’¾ Top 100 sauvegardÃ© en CSV: {output_csv}\n")
    except Exception as e:
        print(f"   âš ï¸  Erreur lors de la sauvegarde CSV: {e}\n")
    
    # ArrÃªter Spark
    spark.stop()
    
    return top_page_tuple, total_pages

@mesurer_temps
def executer_pagerank_dataframe(fichier_input, iterations=10):
    """
    Wrapper avec mesure de temps pour l'exÃ©cution complÃ¨te
    
    Args:
        fichier_input: Chemin GCS vers les donnÃ©es
        iterations: Nombre d'itÃ©rations
    
    Returns:
        Tuple (top_page, total_pages)
    """
    return pagerank_dataframe(fichier_input, iterations)

def main():
    """Fonction principale"""
    
    if len(sys.argv) < 2:
        print("âŒ Usage: spark-submit pagerank_dataframe.py <gs://bucket/data/fichier.ttl> [iterations]")
        print("\nExemple:")
        print("  spark-submit pagerank_dataframe.py gs://mon-bucket/data/wikilinks_10percent.ttl 10")
        sys.exit(1)
    
    fichier = sys.argv[1]
    iterations = int(sys.argv[2]) if len(sys.argv) > 2 else 10
    
    # VÃ©rifier que le fichier est dans GCS
    if not fichier.startswith('gs://'):
        print("âš ï¸  Attention: Le fichier devrait Ãªtre dans Google Cloud Storage (gs://...)")
    
    print("\n" + "ğŸš€" * 40)
    print("DÃ‰MARRAGE DU CALCUL PAGERANK - IMPLÃ‰MENTATION DATAFRAME")
    print("ğŸš€" * 40 + "\n")
    
    # ExÃ©cuter PageRank
    resultat, temps_total = executer_pagerank_dataframe(fichier, iterations)
    top_page, total_pages = resultat
    
    # Afficher le rÃ©sumÃ© final
    print("\n" + "=" * 80)
    print("ğŸ¯ RÃ‰SUMÃ‰ FINAL - PAGERANK DATAFRAME")
    print("=" * 80)
    
    if top_page:
        print(f"ğŸ† CENTRE DE WIKIPEDIA:")
        print(f"   Page: {top_page[0]}")
        print(f"   PageRank: {top_page[1]:.8f}")
    
    print(f"\nğŸ“Š STATISTIQUES:")
    print(f"   Total de pages: {total_pages:,}")
    print(f"   ItÃ©rations: {iterations}")
    print(f"   Temps total: {temps_total:.2f} secondes")
    print(f"   Temps par itÃ©ration: {temps_total/iterations:.2f} secondes")
    
    print("\n" + "=" * 80)
    print("âœ… CALCUL TERMINÃ‰ AVEC SUCCÃˆS")
    print("=" * 80 + "\n")

if __name__ == "__main__":
    main()
