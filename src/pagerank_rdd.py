"""
ImplÃ©mentation PageRank avec PySpark RDD
OptimisÃ© pour Ã©viter le shuffle avec partitionnement intelligent et cache
"""

from pyspark import SparkContext
from pyspark.sql import SparkSession
import sys
import time
from utils import (
    parser_ligne_ttl, 
    calculer_contributions, 
    afficher_top_pagerank,
    afficher_statistiques_graphe,
    afficher_progression,
    mesurer_temps
)

def pagerank_rdd(fichier_input, iterations=10, damping=0.85, num_partitions=200):
    """
    Calcul du PageRank avec PySpark RDD
    
    OPTIMISATIONS CLÃ‰S:
    1. Co-partitionnement des RDDs (liens et rangs)
    2. Cache des donnÃ©es qui ne changent pas
    3. Ã‰vite le shuffle lors des joins
    
    Args:
        fichier_input: Chemin GCS vers les donnÃ©es TTL
        iterations: Nombre d'itÃ©rations PageRank
        damping: Facteur de damping (0.85 par dÃ©faut)
        num_partitions: Nombre de partitions pour le partitionnement
    
    Returns:
        Tuple (top_page, total_pages, rangs_final_rdd)
    """
    
    # CrÃ©er SparkSession
    spark = SparkSession.builder \
        .appName("PageRank-RDD") \
        .config("spark.sql.shuffle.partitions", str(num_partitions)) \
        .config("spark.default.parallelism", str(num_partitions)) \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    
    sc = spark.sparkContext
    sc.setLogLevel("WARN")
    
    print("\n" + "="*80)
    print("ğŸ”´ PageRank avec PySpark RDD")
    print("="*80)
    print(f"ğŸ“ Fichier d'entrÃ©e: {fichier_input}")
    print(f"ğŸ”„ ItÃ©rations: {iterations}")
    print(f"ğŸ“‰ Damping factor: {damping}")
    print(f"ğŸ“¦ Partitions: {num_partitions}")
    print("="*80 + "\n")
    
    # Ã‰tape 1: Charger et parser les donnÃ©es
    print("ğŸ“– Ã‰tape 1/5: Chargement et parsing des donnÃ©es...")
    debut_chargement = time.time()
    
    lignes = sc.textFile(fichier_input)
    
    # Parser les lignes TTL
    liens_bruts = lignes.map(parser_ligne_ttl).filter(lambda x: x is not None)
    
    fin_chargement = time.time()
    print(f"   âœ… Parsing effectuÃ© en {fin_chargement - debut_chargement:.2f} secondes\n")
    
    # Ã‰tape 2: Construire le graphe de liens
    print("ğŸ”— Ã‰tape 2/5: Construction du graphe de liens...")
    debut_graphe = time.time()
    
    # Grouper par source pour avoir (source, [dest1, dest2, ...])
    # OPTIMISATION: Utiliser partitionBy pour co-localiser les donnÃ©es
    liens = liens_bruts.groupByKey() \
        .mapValues(list) \
        .partitionBy(num_partitions) \
        .cache()  # â­ CACHE: Le graphe ne change jamais
    
    # Forcer l'Ã©valuation du cache
    num_liens = liens.count()
    
    fin_graphe = time.time()
    print(f"   âœ… Graphe construit en {fin_graphe - debut_graphe:.2f} secondes")
    
    # Afficher les statistiques
    afficher_statistiques_graphe(liens)
    
    # Ã‰tape 3: Initialiser les rangs PageRank
    print("âš–ï¸  Ã‰tape 3/5: Initialisation des rangs PageRank...")
    
    # OPTIMISATION: Partitionner de la mÃªme maniÃ¨re que les liens
    rangs = liens.map(lambda x: (x[0], 1.0)) \
        .partitionBy(num_partitions)
    
    print(f"   âœ… {num_liens:,} pages initialisÃ©es avec rang = 1.0\n")
    
    # Ã‰tape 4: ItÃ©rations PageRank
    print("ğŸ”„ Ã‰tape 4/5: Calcul PageRank (itÃ©rations)...")
    print("-" * 80)
    
    debut_iterations = time.time()
    
    for iteration in range(iterations):
        # Afficher la progression
        afficher_progression(iteration + 1, iterations)
        
        # OPTIMISATION: Join sans shuffle car mÃªme partitionnement
        # liens et rangs sont partitionnÃ©s de la mÃªme maniÃ¨re (par clÃ©)
        contributions_rdd = liens.join(rangs) \
            .flatMap(lambda url_ranks: calculer_contributions(url_ranks[1][0], url_ranks[1][1]))
        
        # Calculer les nouveaux rangs avec la formule PageRank
        # PageRank(p) = (1-d) + d * Î£(PR(in)/outlinks(in))
        rangs = contributions_rdd.reduceByKey(lambda x, y: x + y) \
            .mapValues(lambda rank: damping * rank + (1 - damping)) \
            .partitionBy(num_partitions)  # Maintenir le partitionnement
    
    fin_iterations = time.time()
    temps_iterations = fin_iterations - debut_iterations
    
    print("-" * 80)
    print(f"âœ… {iterations} itÃ©rations terminÃ©es en {temps_iterations:.2f} secondes")
    print(f"   Temps moyen par itÃ©ration: {temps_iterations/iterations:.2f} secondes\n")
    
    # Ã‰tape 5: RÃ©sultats finaux
    print("ğŸ“Š Ã‰tape 5/5: Calcul des rÃ©sultats finaux...")
    
    # Cache pour les rÃ©sultats finaux
    rangs_final = rangs.cache()
    total_pages = rangs_final.count()
    
    print(f"   âœ… Total de pages analysÃ©es: {total_pages:,}\n")
    
    # Afficher le top 20
    top_page = afficher_top_pagerank(rangs_final, top_n=20)
    
    # Sauvegarder les rÃ©sultats dans GCS
    output_path = fichier_input.replace('/data/', '/results/').replace('.ttl', '_rdd_results')
    print(f"ğŸ’¾ Sauvegarde des rÃ©sultats dans: {output_path}")
    
    try:
        rangs_final.saveAsTextFile(output_path)
        print("   âœ… RÃ©sultats sauvegardÃ©s\n")
    except Exception as e:
        print(f"   âš ï¸  Erreur lors de la sauvegarde: {e}\n")
    
    # ArrÃªter Spark
    spark.stop()
    
    return top_page, total_pages, rangs_final

@mesurer_temps
def executer_pagerank_rdd(fichier_input, iterations=10):
    """
    Wrapper avec mesure de temps pour l'exÃ©cution complÃ¨te
    
    Args:
        fichier_input: Chemin GCS vers les donnÃ©es
        iterations: Nombre d'itÃ©rations
    
    Returns:
        Tuple (top_page, total_pages, rangs_final)
    """
    return pagerank_rdd(fichier_input, iterations)

def main():
    """Fonction principale"""
    
    if len(sys.argv) < 2:
        print("âŒ Usage: spark-submit pagerank_rdd.py <gs://bucket/data/fichier.ttl> [iterations]")
        print("\nExemple:")
        print("  spark-submit pagerank_rdd.py gs://mon-bucket/data/wikilinks_10percent.ttl 10")
        sys.exit(1)
    
    fichier = sys.argv[1]
    iterations = int(sys.argv[2]) if len(sys.argv) > 2 else 10
    
    # VÃ©rifier que le fichier est dans GCS
    if not fichier.startswith('gs://'):
        print("âš ï¸  Attention: Le fichier devrait Ãªtre dans Google Cloud Storage (gs://...)")
    
    print("\n" + "ğŸš€" * 40)
    print("DÃ‰MARRAGE DU CALCUL PAGERANK - IMPLÃ‰MENTATION RDD")
    print("ğŸš€" * 40 + "\n")
    
    # ExÃ©cuter PageRank
    resultat, temps_total = executer_pagerank_rdd(fichier, iterations)
    top_page, total_pages, _ = resultat
    
    # Afficher le rÃ©sumÃ© final
    print("\n" + "=" * 80)
    print("ğŸ¯ RÃ‰SUMÃ‰ FINAL - PAGERANK RDD")
    print("=" * 80)
    
    if top_page:
        print(f"ğŸ† CENTRE DE WIKIPEDIA:")
        print(f"   Page: {top_page[0]}")
        print(f"   PageRank: {top_page[1]:.8f}")
    
    print(f"\nğŸ“Š STATISTIQUES:")
    print(f"   Total de pages: {total_pages:,}")
    print(f"   ItÃ©rations: {iterations}")
    print(f"   Temps total: {temps_total:.2f} secondes")
    print(f"   Temps par itÃ©ration: {temps_total/iterations:.2f} secondes")
    
    print("\n" + "=" * 80)
    print("âœ… CALCUL TERMINÃ‰ AVEC SUCCÃˆS")
    print("=" * 80 + "\n")

if __name__ == "__main__":
    main()
